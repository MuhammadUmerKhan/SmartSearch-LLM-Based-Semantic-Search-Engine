import streamlit as st
import home  # Import the home.py file
from search.google_search import google_custom_search
from search.scraper import extract_full_article
from vector_db.vector_store import create_vector_db
from llm.llm_handler import query_llm

# ğŸ¨ Set Streamlit page configuration
st.set_page_config(page_title="AI Search Engine", page_icon="ğŸ”", layout="wide")

# ğŸ  Sidebar Navigation
st.sidebar.title("ğŸ” AI Search Engine")
page = st.sidebar.radio("ğŸ“Œ Select Page", ["ğŸ  Home", "ğŸ” Search Engine"])

# ğŸ¯ Load Home Page
if page == "ğŸ  Home":
    home.show_home()

# ğŸ” Load Search Engine
elif page == "ğŸ” Search Engine":
    # âœ… Title with gradient effect
    st.markdown("""
        <h1 style="text-align: center;">
            ğŸ” AI-Powered Search Engine with LLMs ğŸ¤–
        </h1>
    """, unsafe_allow_html=True)

    # ğŸ¨ Stylish Search Box
    st.markdown("""
        <style>
            .search-box {
                font-size: 18px;
                padding: 12px;
                border-radius: 8px;
                border: 2px solid #ff4d4d;
                width: 100%;
                background-color: #f9f9f9;
            }
        </style>
    """, unsafe_allow_html=True)

    # âŒ¨ï¸ Real-Time Search Input (Press Enter to Search)
    query = st.text_input("ğŸ” Ask something about AI:", key="search_input")

    # âœ… Perform search automatically when user presses Enter
    if query:
        with st.spinner("â³ Fetching results... Please wait!"):
            try:
                # ğŸ” Google Search API Call
                search_results = google_custom_search(query)

                # ğŸ“„ Extract articles
                all_text = [extract_full_article(result["link"]) for result in search_results]

                # ğŸ§  Create FAISS Vector Database
                vector_db = create_vector_db(all_text)

                # ğŸ” Retrieve relevant chunks
                retrieved_chunks = [doc.page_content for doc in vector_db.similarity_search(query, k=5)]

                # ğŸ¤– Get LLM response
                ai_response = query_llm(query, retrieved_chunks)
                formatted_response = ai_response.content.replace("**", "<b>").replace("**", "</b>")

                st.markdown("<h3 style='color: #ff4d4d;'>ğŸ“Œ AI-Powered Answer:</h3>", unsafe_allow_html=True)
                st.markdown(f"""
                    <div style="
                        padding: 15px;
                        border-radius: 10px;
                        margin-bottom: 20px;
                        border-left: 5px solid #ff4d4d;">
                        <p style="font-size: 18px;">{formatted_response}</p>
                    </div>
                """, unsafe_allow_html=True)
                # ğŸ”— Show Sources in Beautiful Cards
                st.markdown("<h3 style='color: #ff4d4d;'>ğŸ”— Sources:</h3>", unsafe_allow_html=True)
                for result in search_results:
                    st.markdown(f"""
                        <div style="
                            padding: 12px;
                            border-radius: 8px;
                            margin-bottom: 10px;
                            border-left: 5px solid #ff4d4d;
                            ">
                            <b>ğŸ”¹ {result['title']}</b><br>
                            <a href="{result['link']}" target="_blank">{result['link']}</a><br>
                            <i>{result['snippet']}</i>
                        </div>
                    """, unsafe_allow_html=True)

            except Exception as e:
                st.error(f"âš ï¸ An error occurred: {str(e)}")
