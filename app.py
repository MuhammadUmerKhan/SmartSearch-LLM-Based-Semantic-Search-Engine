import streamlit as st
from search.google_search import google_custom_search
from search.scraper import extract_full_article
from vector_db.vector_store import create_vector_db
from llm.llm_handler import query_llm
from pages import instruct, home

# ğŸ¨ Set Streamlit page configuration
st.set_page_config(page_title="AI Search Engine", page_icon="ğŸ”", layout="wide")

available_llms = {
    "Llama": "llama-3.3-70b-versatile",
    "Gemma": "gemma2-9b-it",
    "Qwen 2.5": "qwen-2.5-32b",
    "DeepSeek R1 32b": "deepseek-r1-distill-qwen-32b",
    "DeepSeek R1 70b": "deepseek-r1-distill-llama-70b",
    "DeepSeek Qwen": "deepseek-r1-distill-qwen-32b"
}

# ğŸŒŸ **Sidebar: Select LLM**
selected_llm = st.sidebar.selectbox("ğŸ¤– Select an LLM Model", list(available_llms.keys()))

# Store the selected LLM in session state
st.session_state["selected_llm"] = available_llms[selected_llm]

# ğŸ  Sidebar Navigation
st.sidebar.title("ğŸ” AI Search Engine")
page = st.sidebar.radio("ğŸ“Œ Select Page", ["ğŸ  Home", "ğŸ” Search Engine"])

# ğŸ¯ Load Home Page
if page == "ğŸ  Home":
    home.show_home()

# ğŸ” Load Search Engine
elif page == "ğŸ” Search Engine":
    # ğŸ¯ **Navigation Instructions**
    instruct.instruct()

    # âŒ¨ï¸ Real-Time Search Input (Press Enter to Search)
    query = st.text_input("ğŸ” Ask Anything:", key="search_input", placeholder="Enter your query")

    # âœ… Perform search automatically when user presses Enter
    if query:
        with st.spinner("â³ Fetching results... Please wait!"):
            try:
                # ğŸ” Google Search API Call
                search_results = google_custom_search(query)

                # ğŸ“„ Extract articles
                all_text = [extract_full_article(result["link"]) for result in search_results]

                # ğŸ§  Create FAISS Vector Database
                vector_db = create_vector_db(all_text)

                # ğŸ” Retrieve relevant chunks
                retrieved_chunks = [doc.page_content for doc in vector_db.similarity_search(query, k=5)]

                # ğŸ¤– Get LLM response
                ai_response = query_llm(query, retrieved_chunks, model_name=st.session_state["selected_llm"])
                formatted_response = ai_response.content.replace("**", "<b>").replace("**", "</b>")

                st.markdown("<h3 style='color: #ff4d4d;'>ğŸ“Œ AI-Powered Answer:</h3>", unsafe_allow_html=True)
                st.markdown(f"""
                    <div style="
                        padding: 15px;
                        border-radius: 10px;
                        margin-bottom: 20px;
                        border-left: 5px solid #ff4d4d;">
                        <p style="font-size: 18px;">{formatted_response}</p>
                    </div>
                """, unsafe_allow_html=True)
                # ğŸ”— Show Sources in Beautiful Cards
                st.markdown("<h3 style='color: #ff4d4d;'>ğŸ”— Sources:</h3>", unsafe_allow_html=True)
                for result in search_results:
                    st.markdown(f"""
                        <div style="
                            padding: 12px;
                            border-radius: 8px;
                            margin-bottom: 10px;
                            border-left: 5px solid #ff4d4d;
                            ">
                            <b>ğŸ”¹ {result['title']}</b><br>
                            <a href="{result['link']}" target="_blank">{result['link']}</a><br>
                            <i>{result['snippet']}</i>
                        </div>
                    """, unsafe_allow_html=True)

            except Exception as e:
                st.error(f"âš ï¸ An error occurred: {str(e)}")
