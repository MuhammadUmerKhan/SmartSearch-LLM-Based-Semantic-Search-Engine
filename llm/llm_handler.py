from langchain_groq import ChatGroq
from config.config import GROQ_API_KEY
import logging

# Logging configuration
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def query_llm(query, retrieved_chunks):
    """
    Generates a structured response using LLM.

    Args:
        query (str): User query.
        retrieved_chunks (list): Retrieved document chunks.

    Returns:
        str: AI-generated structured response.
    """
    try:
        logging.info("ğŸ¤– Querying LLM...")
        llm = ChatGroq(
            temperature=0,
            groq_api_key=GROQ_API_KEY,
            model_name="llama-3.3-70b-versatile"
        )

        # Combine retrieved text chunks
        context_text = "\n".join(retrieved_chunks)

        prompt = f"""
        ğŸ¯ You are an **AI expert** providing **concise, well-structured, and engaging** responses.

        ğŸ” **User Query:** {query}

        ğŸ” **Extracted Information from Trusted Sources:** 
        {context_text}

        âœ¨ **Response Guidelines:**  
        - Use **structured bullet points** âœ…  
        - Highlight **key facts** with **emojis** ğŸ¯  
        - Keep it **concise yet highly informative** ğŸ“Œ  
        - **No unnecessary filler text**â€”focus on **value-driven insights** ğŸš€  
        - Maintain a **professional yet engaging** tone ğŸ¤  
        - End with a **brief but powerful conclusion** âœï¸  

        Now, generate the structured response using **emojis** to enhance clarity and engagement.  
        """
        
        response = llm.invoke(prompt)
        logging.info("âœ… LLM Response Generated Successfully.")
        return response

    except Exception as e:
        logging.error(f"âŒ LLM Query Error: {str(e)}")
        return "âŒ Error generating LLM response."
